{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Real images: 74850\n",
      "‚úÖ Fake images: 527423\n",
      "‚úÖ Optical Flow images: 74849\n",
      "‚úÖ Edge images: 74849\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define your dataset paths\n",
    "output_real_faces = \"E:/dataset_1/disgust/real\"\n",
    "output_fake_faces = \"E:/dataset_1/happy/fake\"\n",
    "output_optical_flow = \"E:/dataset_1/disgust/optical_flow\"\n",
    "output_edges = \"E:/dataset_1/disgust/edges\"\n",
    "\n",
    "# Check number of files in each folder\n",
    "real_face_files = len([f for f in os.listdir(output_real_faces) if f.endswith(\".jpg\")])\n",
    "fake_face_files = len([f for f in os.listdir(output_fake_faces) if f.endswith(\".jpg\")])\n",
    "flow_files = len([f for f in os.listdir(output_optical_flow) if f.endswith(\".jpg\")])\n",
    "edge_files = len([f for f in os.listdir(output_edges) if f.endswith(\".jpg\")])\n",
    "\n",
    "print(f\"‚úÖ Real images: {real_face_files}\")\n",
    "print(f\"‚úÖ Fake images: {fake_face_files}\")\n",
    "print(f\"‚úÖ Optical Flow images: {flow_files}\")\n",
    "print(f\"‚úÖ Edge images: {edge_files}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Real face images: 74850\n",
      "‚úÖ Optical Flow images: 74849 (Expected: 74850)\n",
      "‚úÖ Edge images: 74849 (Expected: 74850)\n",
      "\n",
      "‚ùå Missing Optical Flow for 1 faces:\n",
      "camera_right_M133_light_right_disgust_camera_right.mp4_frame12_face0.jpg\n",
      "\n",
      "‚ùå Missing Edge Maps for 1 faces:\n",
      "camera_right_M133_light_right_disgust_camera_right.mp4_frame12_face0.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define dataset paths\n",
    "output_real_faces = \"E:/dataset_1/disgust/real\"\n",
    "output_fake_faces = \"E:/dataset_1/happy/fake\"\n",
    "output_optical_flow = \"E:/dataset_1/disgust/optical_flow\"\n",
    "output_edges = \"E:/dataset_1/disgust/edges\"\n",
    "\n",
    "# Get face filenames (keep full names without modification)\n",
    "real_face_files = {f for f in os.listdir(output_real_faces) if f.endswith(\".jpg\")}\n",
    "# fake_face_files = {f for f in os.listdir(output_fake_faces) if f.endswith(\".jpg\")}\n",
    "\n",
    "# Combine real and fake face filenames\n",
    "total_faces = real_face_files\n",
    "\n",
    "# Get optical flow and edge filenames (keep full names)\n",
    "optical_flows = {f.replace(\"_flow.jpg\", \".jpg\") for f in os.listdir(output_optical_flow) if f.endswith(\".jpg\")}\n",
    "edge_maps = {f.replace(\"_edges.jpg\", \".jpg\") for f in os.listdir(output_edges) if f.endswith(\".jpg\")}\n",
    "\n",
    "# Find missing optical flow and edge maps\n",
    "missing_flows = total_faces - optical_flows\n",
    "missing_edges = total_faces - edge_maps\n",
    "\n",
    "# Print dataset counts\n",
    "print(f\"‚úÖ Real face images: {len(real_face_files)}\")\n",
    "# print(f\"‚úÖ Fake face images: {len(fake_face_files)}\")\n",
    "print(f\"‚úÖ Optical Flow images: {len(optical_flows)} (Expected: {len(total_faces)})\")\n",
    "print(f\"‚úÖ Edge images: {len(edge_maps)} (Expected: {len(total_faces)})\")\n",
    "\n",
    "# Print missing files (if any)\n",
    "if missing_flows:\n",
    "    print(f\"\\n‚ùå Missing Optical Flow for {len(missing_flows)} faces:\")\n",
    "    print(\"\\n\".join(list(missing_flows)[:5]))  # Show first 5 missing\n",
    "\n",
    "if missing_edges:\n",
    "    print(f\"\\n‚ùå Missing Edge Maps for {len(missing_edges)} faces:\")\n",
    "    print(\"\\n\".join(list(missing_edges)[:5]))  # Show first 5 missing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real samples: 10000, Fake samples: 10000, Total: 20000\n",
      "üî® Creating memmap files...\n",
      "\n",
      "üü¢ Processing REAL images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing E:/dataset_1/disgust/real: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [29:20<00:00,  5.68img/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî¥ Processing FAKE images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing E:/dataset_1/happy/fake: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [1:45:46<00:00,  1.58img/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Done processing.\n",
      "Real written: 10000, Fake written: 10000, Total written: 20000\n",
      "üíæ Creating final compressed NPZ file...\n",
      "‚úÖ Data saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define dataset paths\n",
    "output_real_faces = \"E:/dataset_1/disgust/real\"\n",
    "output_optical_flow = \"E:/dataset_1/disgust/optical_flow\" \n",
    "output_edges = \"E:/dataset_1/disgust/edges\"\n",
    "\n",
    "output_fake_faces = \"E:/dataset_1/happy/fake\"\n",
    "output_fake_optical_flow = \"E:/dataset_1/happy/optical_flow\"\n",
    "output_fake_edges = \"E:/dataset_1/happy/edges\"\n",
    "\n",
    "save_path = \"E:/dataset_1/preprocessed_disgust.npz\"\n",
    "image_shape = (299, 299, 9)\n",
    "batch_size = 1000  # Number of images to load/write in one batch\n",
    "\n",
    "def get_valid_files(face_dir, flow_dir, edge_dir, max_samples=None):\n",
    "    \"\"\"Get a shuffled list of valid filenames (face, flow, edges) all exist.\"\"\"\n",
    "    face_files = [f for f in os.listdir(face_dir) if f.endswith(\".jpg\")]\n",
    "    random.shuffle(face_files)\n",
    "    if max_samples is not None:\n",
    "        face_files = face_files[:max_samples]\n",
    "    return face_files\n",
    "\n",
    "def process_and_write_to_memmap(face_dir, flow_dir, edge_dir, face_files, \n",
    "                                x_memmap, y_memmap, label, start_idx=0, \n",
    "                                batch_size=1000):\n",
    "    \"\"\"\n",
    "    Loads images in batches, writes directly to memmap to avoid storing in RAM.\n",
    "    Returns the number of images actually written.\n",
    "    \"\"\"\n",
    "    total_written = 0\n",
    "\n",
    "    # Create a TQDM progress bar for the entire list of files\n",
    "    pbar = tqdm(total=len(face_files), desc=f\"Processing {face_dir}\", unit=\"img\")\n",
    "\n",
    "    for i in range(0, len(face_files), batch_size):\n",
    "        batch_files = face_files[i:i + batch_size]\n",
    "\n",
    "        # We'll collect a batch in memory just for the current chunk\n",
    "        batch_x = []\n",
    "        batch_y = []\n",
    "\n",
    "        for face_file in batch_files:\n",
    "            base_name = face_file.replace(\".jpg\", \"\")\n",
    "\n",
    "            face_path = os.path.join(face_dir, face_file)\n",
    "            flow_path = os.path.join(flow_dir, base_name + \"_flow.jpg\")\n",
    "            edge_path = os.path.join(edge_dir, base_name + \"_edges.jpg\")\n",
    "\n",
    "            if (os.path.exists(face_path) and \n",
    "                os.path.exists(flow_path) and \n",
    "                os.path.exists(edge_path)):\n",
    "\n",
    "                try:\n",
    "                    face_img = cv2.imread(face_path)\n",
    "                    flow_img = cv2.imread(flow_path)\n",
    "                    edge_img = cv2.imread(edge_path)\n",
    "\n",
    "                    # If any read fails, skip it\n",
    "                    if face_img is None or flow_img is None or edge_img is None:\n",
    "                        pbar.update(1)\n",
    "                        continue\n",
    "\n",
    "                    face_img = cv2.resize(face_img, (299, 299))\n",
    "                    flow_img = cv2.resize(flow_img, (299, 299))\n",
    "                    edge_img = cv2.resize(edge_img, (299, 299))\n",
    "\n",
    "                    # Normalize\n",
    "                    face_img = face_img.astype(np.float32) / 255.0\n",
    "                    flow_img = flow_img.astype(np.float32) / 255.0\n",
    "                    edge_img = edge_img.astype(np.float32) / 255.0\n",
    "\n",
    "                    # Concatenate into 9 channels\n",
    "                    combined_input = np.concatenate((face_img, flow_img, edge_img), axis=-1)\n",
    "                    \n",
    "                    batch_x.append(combined_input)\n",
    "                    batch_y.append(label)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Error processing {face_file}: {e}\")\n",
    "\n",
    "            # Update TQDM for *every* file we attempt\n",
    "            pbar.update(1)\n",
    "\n",
    "        # Write the current batch to memmap\n",
    "        if batch_x:\n",
    "            current_batch_size = len(batch_x)\n",
    "            x_memmap[start_idx : start_idx + current_batch_size] = batch_x\n",
    "            y_memmap[start_idx : start_idx + current_batch_size] = batch_y\n",
    "            start_idx += current_batch_size\n",
    "            total_written += current_batch_size\n",
    "\n",
    "    pbar.close()\n",
    "    return total_written\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 1. Gather file lists and determine total size\n",
    "# ---------------------------------------------------------------------------------\n",
    "real_face_files = get_valid_files(output_real_faces, output_optical_flow, output_edges, max_samples=10000)\n",
    "fake_face_files = get_valid_files(output_fake_faces, output_fake_optical_flow, output_fake_edges, max_samples=10000)\n",
    "\n",
    "num_real = len(real_face_files)\n",
    "num_fake = len(fake_face_files)\n",
    "total_samples = num_real + num_fake\n",
    "\n",
    "print(f\"Real samples: {num_real}, Fake samples: {num_fake}, Total: {total_samples}\")\n",
    "if total_samples == 0:\n",
    "    print(\"‚ùå ERROR: No valid images found! Exiting.\")\n",
    "    exit()\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 2. Create memmap arrays (one big array for real+fake)\n",
    "# ---------------------------------------------------------------------------------\n",
    "x_data_memmap_path = save_path.replace(\".npz\", \"_x.dat\")\n",
    "y_data_memmap_path = save_path.replace(\".npz\", \"_y.dat\")\n",
    "\n",
    "print(\"üî® Creating memmap files...\")\n",
    "x_data_memmap = np.memmap(\n",
    "    x_data_memmap_path, \n",
    "    dtype=np.float32, \n",
    "    mode=\"w+\", \n",
    "    shape=(total_samples, *image_shape)\n",
    ")\n",
    "y_data_memmap = np.memmap(\n",
    "    y_data_memmap_path,\n",
    "    dtype=np.int8,\n",
    "    mode=\"w+\",\n",
    "    shape=(total_samples,)\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 3. Process REAL samples in chunks\n",
    "# ---------------------------------------------------------------------------------\n",
    "print(\"\\nüü¢ Processing REAL images...\")\n",
    "current_index = 0\n",
    "written_real = process_and_write_to_memmap(\n",
    "    output_real_faces, \n",
    "    output_optical_flow, \n",
    "    output_edges, \n",
    "    real_face_files, \n",
    "    x_data_memmap, \n",
    "    y_data_memmap, \n",
    "    label=0, \n",
    "    start_idx=current_index,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "current_index += written_real\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 4. Process FAKE samples in chunks\n",
    "# ---------------------------------------------------------------------------------\n",
    "print(\"\\nüî¥ Processing FAKE images...\")\n",
    "written_fake = process_and_write_to_memmap(\n",
    "    output_fake_faces, \n",
    "    output_fake_optical_flow, \n",
    "    output_fake_edges, \n",
    "    fake_face_files, \n",
    "    x_data_memmap, \n",
    "    y_data_memmap, \n",
    "    label=1, \n",
    "    start_idx=current_index,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "current_index += written_fake\n",
    "\n",
    "print(f\"\\n‚úÖ Done processing.\\nReal written: {written_real}, Fake written: {written_fake}, Total written: {written_real + written_fake}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 5. Flush memmap to disk and create compressed .npz\n",
    "# ---------------------------------------------------------------------------------\n",
    "del x_data_memmap\n",
    "del y_data_memmap\n",
    "\n",
    "print(\"üíæ Creating final compressed NPZ file...\")\n",
    "\n",
    "# Reopen in read mode and save as NPZ\n",
    "x_data_memmap = np.memmap(\n",
    "    x_data_memmap_path, \n",
    "    dtype=np.float32, \n",
    "    mode=\"r\", \n",
    "    shape=(written_real + written_fake, *image_shape)\n",
    ")\n",
    "y_data_memmap = np.memmap(\n",
    "    y_data_memmap_path, \n",
    "    dtype=np.int8, \n",
    "    mode=\"r\", \n",
    "    shape=(written_real + written_fake,)\n",
    ")\n",
    "\n",
    "np.savez_compressed(\n",
    "    save_path, \n",
    "    x=x_data_memmap, \n",
    "    y=y_data_memmap\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Data saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading precomputed dataset from E:/dataset_1/preprocessed_sad.npz...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mE:/dataset_1/preprocessed_sad.npz\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müìÇ Loading precomputed dataset from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(save_path)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Load precomputed dataset\n",
    "save_path = \"E:/dataset_1/preprocessed_sad.npz\"\n",
    "print(f\"üìÇ Loading precomputed dataset from {save_path}...\")\n",
    "data = np.load(save_path)\n",
    "\n",
    "# # Extract x_data and y_data\n",
    "# x_data, y_data = data[\"x\"], data[\"y\"]\n",
    "# print(f\"‚úÖ Loaded dataset: {x_data.shape[0]} samples.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
