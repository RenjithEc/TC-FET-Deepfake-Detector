{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Planned real samples: 80000, Planned fake samples: 80000, Total: 160000\n",
      "🔨 Creating memmap files...\n",
      "\n",
      "🟢 Processing REAL images from: E:/dataset_1/sad/real\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing E:/dataset_1/sad/real: 100%|██████████| 10000/10000 [18:18<00:00,  9.10img/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🟢 Processing REAL images from: E:/dataset_1/angry/real\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing E:/dataset_1/angry/real: 100%|██████████| 10000/10000 [14:36<00:00, 11.40img/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🟢 Processing REAL images from: E:/dataset_1/contempt/real\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing E:/dataset_1/contempt/real: 100%|██████████| 10000/10000 [15:32<00:00, 10.73img/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🟢 Processing REAL images from: E:/dataset_1/disgust/real\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing E:/dataset_1/disgust/real: 100%|██████████| 10000/10000 [13:35<00:00, 12.26img/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🟢 Processing REAL images from: E:/dataset_1/fear/real\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing E:/dataset_1/fear/real: 100%|██████████| 10000/10000 [17:55<00:00,  9.30img/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🟢 Processing REAL images from: E:/dataset_1/happy/real\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing E:/dataset_1/happy/real: 100%|██████████| 10000/10000 [34:17<00:00,  4.86img/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🟢 Processing REAL images from: E:/dataset_1/neutral/real\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing E:/dataset_1/neutral/real: 100%|██████████| 10000/10000 [22:14<00:00,  7.49img/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🟢 Processing REAL images from: E:/dataset_1/surprise/real\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing E:/dataset_1/surprise/real: 100%|██████████| 10000/10000 [15:36<00:00, 10.67img/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Finished processing REAL images. Total real images written: 79999\n",
      "\n",
      "🔴 Processing FAKE images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing E:/dataset_1/happy/fake: 100%|██████████| 80000/80000 [7:59:27<00:00,  2.78img/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Finished processing FAKE images. Total fake images written: 80000\n",
      "\n",
      "✅ Overall, images written: 159999 out of planned 160000\n",
      "💾 Creating final compressed NPZ file...\n",
      "✅ Data saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define dataset paths for real emotions\n",
    "output_real_faces_sad = \"E:/dataset_1/sad/real\"\n",
    "output_optical_flow_sad = \"E:/dataset_1/sad/optical_flow\" \n",
    "output_edges_sad = \"E:/dataset_1/sad/edges\"\n",
    "\n",
    "output_real_faces_angry = \"E:/dataset_1/angry/real\"\n",
    "output_optical_flow_angry = \"E:/dataset_1/angry/optical_flow\" \n",
    "output_edges_angry = \"E:/dataset_1/angry/edges\"\n",
    "\n",
    "output_real_faces_contempt = \"E:/dataset_1/contempt/real\"\n",
    "output_optical_flow_contempt = \"E:/dataset_1/contempt/optical_flow\" \n",
    "output_edges_contempt = \"E:/dataset_1/contempt/edges\"\n",
    "\n",
    "output_real_faces_disgust = \"E:/dataset_1/disgust/real\"\n",
    "output_optical_flow_disgust = \"E:/dataset_1/disgust/optical_flow\" \n",
    "output_edges_disgust = \"E:/dataset_1/disgust/edges\"\n",
    "\n",
    "output_real_faces_fear = \"E:/dataset_1/fear/real\"\n",
    "output_optical_flow_fear = \"E:/dataset_1/fear/optical_flow\" \n",
    "output_edges_fear = \"E:/dataset_1/fear/edges\"\n",
    "\n",
    "output_real_faces_happy = \"E:/dataset_1/happy/real\"\n",
    "output_optical_flow_happy = \"E:/dataset_1/happy/optical_flow\" \n",
    "output_edges_happy = \"E:/dataset_1/happy/edges\"\n",
    "\n",
    "output_real_faces_neutral = \"E:/dataset_1/neutral/real\"\n",
    "output_optical_flow_neutral = \"E:/dataset_1/neutral/optical_flow\"\n",
    "output_edges_neutral = \"E:/dataset_1/neutral/edges\"\n",
    "\n",
    "output_real_faces_surprise = \"E:/dataset_1/surprise/real\"\n",
    "output_optical_flow_surprise = \"E:/dataset_1/surprise/optical_flow\" \n",
    "output_edges_surprise = \"E:/dataset_1/surprise/edges\"\n",
    "\n",
    "# Define dataset paths for fake images \n",
    "output_fake_faces = \"E:/dataset_1/happy/fake\"\n",
    "output_fake_optical_flow = \"E:/dataset_1/happy/optical_flow\"\n",
    "output_fake_edges = \"E:/dataset_1/happy/edges\"\n",
    "\n",
    "save_path = \"E:/dataset_1/preprocessed_combined.npz\"\n",
    "image_shape = (299, 299, 9)\n",
    "batch_size = 1000  # Number of images to load/write in one batch\n",
    "\n",
    "def get_valid_files(face_dir, flow_dir, edge_dir, max_samples=None):\n",
    "    \"\"\"Get a shuffled list of valid filenames (face, flow, edges) that exist.\"\"\"\n",
    "    face_files = [f for f in os.listdir(face_dir) if f.endswith(\".jpg\")]\n",
    "    random.shuffle(face_files)\n",
    "    if max_samples is not None:\n",
    "        face_files = face_files[:max_samples]\n",
    "    return face_files\n",
    "\n",
    "def process_and_write_to_memmap(face_dir, flow_dir, edge_dir, face_files, \n",
    "                                x_memmap, y_memmap, label, start_idx=0, \n",
    "                                batch_size=1000):\n",
    "    \"\"\"\n",
    "    Loads images in batches, writes directly to memmap to avoid storing in RAM.\n",
    "    Returns the number of images actually written.\n",
    "    \"\"\"\n",
    "    total_written = 0\n",
    "    pbar = tqdm(total=len(face_files), desc=f\"Processing {face_dir}\", unit=\"img\")\n",
    "\n",
    "    for i in range(0, len(face_files), batch_size):\n",
    "        batch_files = face_files[i:i + batch_size]\n",
    "        batch_x = []\n",
    "        batch_y = []\n",
    "\n",
    "        for face_file in batch_files:\n",
    "            base_name = face_file.replace(\".jpg\", \"\")\n",
    "            face_path = os.path.join(face_dir, face_file)\n",
    "            flow_path = os.path.join(flow_dir, base_name + \"_flow.jpg\")\n",
    "            edge_path = os.path.join(edge_dir, base_name + \"_edges.jpg\")\n",
    "\n",
    "            if (os.path.exists(face_path) and \n",
    "                os.path.exists(flow_path) and \n",
    "                os.path.exists(edge_path)):\n",
    "\n",
    "                try:\n",
    "                    face_img = cv2.imread(face_path)\n",
    "                    flow_img = cv2.imread(flow_path)\n",
    "                    edge_img = cv2.imread(edge_path)\n",
    "\n",
    "                    # Skip if any image failed to load\n",
    "                    if face_img is None or flow_img is None or edge_img is None:\n",
    "                        pbar.update(1)\n",
    "                        continue\n",
    "\n",
    "                    face_img = cv2.resize(face_img, (299, 299))\n",
    "                    flow_img = cv2.resize(flow_img, (299, 299))\n",
    "                    edge_img = cv2.resize(edge_img, (299, 299))\n",
    "\n",
    "                    # Normalize images\n",
    "                    face_img = face_img.astype(np.float32) / 255.0\n",
    "                    flow_img = flow_img.astype(np.float32) / 255.0\n",
    "                    edge_img = edge_img.astype(np.float32) / 255.0\n",
    "\n",
    "                    # Concatenate into 9 channels\n",
    "                    combined_input = np.concatenate((face_img, flow_img, edge_img), axis=-1)\n",
    "                    \n",
    "                    batch_x.append(combined_input)\n",
    "                    batch_y.append(label)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ Error processing {face_file}: {e}\")\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "        if batch_x:\n",
    "            current_batch_size = len(batch_x)\n",
    "            x_memmap[start_idx : start_idx + current_batch_size] = batch_x\n",
    "            y_memmap[start_idx : start_idx + current_batch_size] = batch_y\n",
    "            start_idx += current_batch_size\n",
    "            total_written += current_batch_size\n",
    "\n",
    "    pbar.close()\n",
    "    return total_written\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 1. Define real emotion categories and compute total samples\n",
    "# ---------------------------------------------------------------------------------\n",
    "real_emotions = [\n",
    "    (output_real_faces_sad, output_optical_flow_sad, output_edges_sad),\n",
    "    (output_real_faces_angry, output_optical_flow_angry, output_edges_angry),\n",
    "    (output_real_faces_contempt, output_optical_flow_contempt, output_edges_contempt),\n",
    "    (output_real_faces_disgust, output_optical_flow_disgust, output_edges_disgust),\n",
    "    (output_real_faces_fear, output_optical_flow_fear, output_edges_fear),\n",
    "    (output_real_faces_happy, output_optical_flow_happy, output_edges_happy),\n",
    "    (output_real_faces_neutral, output_optical_flow_neutral, output_edges_neutral),\n",
    "    (output_real_faces_surprise, output_optical_flow_surprise, output_edges_surprise)\n",
    "]\n",
    "\n",
    "max_real_per_emotion = 10000\n",
    "max_fake = 80000\n",
    "max_real_total = max_real_per_emotion * len(real_emotions)\n",
    "total_samples = max_real_total + max_fake\n",
    "\n",
    "print(f\"Planned real samples: {max_real_total}, Planned fake samples: {max_fake}, Total: {total_samples}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 2. Create memmap arrays for all samples\n",
    "# ---------------------------------------------------------------------------------\n",
    "x_data_memmap_path = save_path.replace(\".npz\", \"_x.dat\")\n",
    "y_data_memmap_path = save_path.replace(\".npz\", \"_y.dat\")\n",
    "\n",
    "print(\"🔨 Creating memmap files...\")\n",
    "x_data_memmap = np.memmap(\n",
    "    x_data_memmap_path, \n",
    "    dtype=np.float32, \n",
    "    mode=\"w+\", \n",
    "    shape=(total_samples, *image_shape)\n",
    ")\n",
    "y_data_memmap = np.memmap(\n",
    "    y_data_memmap_path,\n",
    "    dtype=np.int8,\n",
    "    mode=\"w+\",\n",
    "    shape=(total_samples,)\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 3. Process REAL samples for each emotion category\n",
    "# ---------------------------------------------------------------------------------\n",
    "current_index = 0\n",
    "total_real_written = 0\n",
    "for face_dir, flow_dir, edge_dir in real_emotions:\n",
    "    print(f\"\\n🟢 Processing REAL images from: {face_dir}\")\n",
    "    face_files = get_valid_files(face_dir, flow_dir, edge_dir, max_samples=max_real_per_emotion)\n",
    "    written = process_and_write_to_memmap(\n",
    "        face_dir, \n",
    "        flow_dir, \n",
    "        edge_dir, \n",
    "        face_files, \n",
    "        x_data_memmap, \n",
    "        y_data_memmap, \n",
    "        label=0, \n",
    "        start_idx=current_index,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    current_index += written\n",
    "    total_real_written += written\n",
    "\n",
    "print(f\"\\n✅ Finished processing REAL images. Total real images written: {total_real_written}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 4. Process FAKE samples in one chunk\n",
    "# ---------------------------------------------------------------------------------\n",
    "print(\"\\n🔴 Processing FAKE images...\")\n",
    "fake_face_files = get_valid_files(output_fake_faces, output_fake_optical_flow, output_fake_edges, max_samples=max_fake)\n",
    "written_fake = process_and_write_to_memmap(\n",
    "    output_fake_faces, \n",
    "    output_fake_optical_flow, \n",
    "    output_fake_edges, \n",
    "    fake_face_files, \n",
    "    x_data_memmap, \n",
    "    y_data_memmap, \n",
    "    label=1, \n",
    "    start_idx=current_index,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "current_index += written_fake\n",
    "\n",
    "print(f\"\\n✅ Finished processing FAKE images. Total fake images written: {written_fake}\")\n",
    "\n",
    "print(f\"\\n✅ Overall, images written: {total_real_written + written_fake} out of planned {total_samples}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 5. Flush memmap to disk and create compressed .npz file\n",
    "# ---------------------------------------------------------------------------------\n",
    "del x_data_memmap\n",
    "del y_data_memmap\n",
    "\n",
    "print(\"💾 Creating final compressed NPZ file...\")\n",
    "\n",
    "# Reopen memmap arrays in read mode for the actual number of images written\n",
    "x_data_memmap = np.memmap(\n",
    "    x_data_memmap_path, \n",
    "    dtype=np.float32, \n",
    "    mode=\"r\", \n",
    "    shape=(total_real_written + written_fake, *image_shape)\n",
    ")\n",
    "y_data_memmap = np.memmap(\n",
    "    y_data_memmap_path, \n",
    "    dtype=np.int8, \n",
    "    mode=\"r\", \n",
    "    shape=(total_real_written + written_fake,)\n",
    ")\n",
    "\n",
    "np.savez_compressed(\n",
    "    save_path, \n",
    "    x=x_data_memmap, \n",
    "    y=y_data_memmap\n",
    ")\n",
    "\n",
    "print(\"✅ Data saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading memmap arrays from existing .dat files...\n",
      "Saving data as uncompressed NPY files...\n",
      "✅ Data saved successfully as uncompressed NPY files:\n",
      "    E:/dataset_1/combined_x_data.npy\n",
      "    E:/dataset_1/combined_y_data.npy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Paths to your existing .dat files (from your previous run)\n",
    "x_dat_path = \"E:/dataset_1/preprocessed_combined_x.dat\"\n",
    "y_dat_path = \"E:/dataset_1/preprocessed_combined_y.dat\"\n",
    "\n",
    "# Define the shape of your data.\n",
    "# Replace num_samples with the actual number of samples written (e.g., total_real_written + written_fake)\n",
    "num_samples = 123456  # <-- Update this value accordingly\n",
    "image_shape = (299, 299, 9)\n",
    "\n",
    "print(\"Loading memmap arrays from existing .dat files...\")\n",
    "x_memmap = np.memmap(x_dat_path, dtype=np.float32, mode=\"r\", shape=(num_samples, *image_shape))\n",
    "y_memmap = np.memmap(y_dat_path, dtype=np.int8, mode=\"r\", shape=(num_samples,))\n",
    "\n",
    "# Define paths for the uncompressed NPY files\n",
    "x_save_path = \"E:/dataset_1/combined_x_data.npy\"\n",
    "y_save_path = \"E:/dataset_1/combined_y_data.npy\"\n",
    "\n",
    "print(\"Saving data as uncompressed NPY files...\")\n",
    "np.save(x_save_path, x_memmap)\n",
    "np.save(y_save_path, y_memmap)\n",
    "\n",
    "print(\"✅ Data saved successfully as uncompressed NPY files:\")\n",
    "print(f\"    {x_save_path}\")\n",
    "print(f\"    {y_save_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
